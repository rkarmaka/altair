{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Export for Deployment\n",
    "\n",
    "This notebook demonstrates how to export trained models for deployment.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to export models to ONNX format\n",
    "- How to export models to TorchScript format\n",
    "- How to validate exported models\n",
    "- How to run inference with exported models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Formats\n",
    "\n",
    "Altair supports two export formats:\n",
    "\n",
    "| Format | Extension | Use Case |\n",
    "|--------|-----------|----------|\n",
    "| **ONNX** | `.onnx` | Cross-platform (TensorRT, OpenVINO, ONNX Runtime) |\n",
    "| **TorchScript** | `.pt` | PyTorch ecosystem (LibTorch C++, mobile) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Export\n",
    "\n",
    "The simplest way to export a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX\n",
    "# path = alt.export(\"run_abc123\", \"model.onnx\")\n",
    "# print(f\"Exported to: {path}\")\n",
    "\n",
    "# Export to TorchScript\n",
    "# path = alt.export(\"run_abc123\", \"model.pt\", format=\"torchscript\")\n",
    "# print(f\"Exported to: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Export\n",
    "\n",
    "ONNX is ideal for cross-platform deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_export_code = \"\"\"\n",
    "# Basic ONNX export\n",
    "path = alt.export(\"run_abc123\", \"model.onnx\")\n",
    "\n",
    "# With custom options\n",
    "path = alt.export(\n",
    "    \"run_abc123\",\n",
    "    \"model.onnx\",\n",
    "    format=\"onnx\",\n",
    "    input_shape=(1, 3, 512, 512),  # Batch, Channels, Height, Width\n",
    "    opset_version=17,               # ONNX opset version\n",
    "    dynamic_axes=True,              # Allow variable batch/spatial size\n",
    "    simplify=True,                  # Simplify the graph (requires onnxsim)\n",
    "    validate=True,                  # Validate output matches PyTorch\n",
    ")\n",
    "\"\"\"\n",
    "print(onnx_export_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic vs Fixed Input Size\n",
    "\n",
    "By default, ONNX exports support dynamic input sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_code = \"\"\"\n",
    "# Dynamic axes (default) - supports any input size\n",
    "path = alt.export(\"run_abc123\", \"model_dynamic.onnx\", dynamic_axes=True)\n",
    "\n",
    "# Fixed input size - slightly faster but inflexible\n",
    "path = alt.export(\n",
    "    \"run_abc123\",\n",
    "    \"model_fixed.onnx\",\n",
    "    input_shape=(1, 3, 512, 512),\n",
    "    dynamic_axes=False,\n",
    ")\n",
    "\"\"\"\n",
    "print(dynamic_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript Export\n",
    "\n",
    "TorchScript is ideal for PyTorch ecosystem deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_export_code = \"\"\"\n",
    "# Basic TorchScript export\n",
    "path = alt.export(\"run_abc123\", \"model.pt\", format=\"torchscript\")\n",
    "\n",
    "# The model is exported using tracing by default\n",
    "# and optimized for inference\n",
    "\"\"\"\n",
    "print(ts_export_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the ModelExporter Class\n",
    "\n",
    "For more control, use the `ModelExporter` class directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporter_code = \"\"\"\n",
    "from altair.export import ModelExporter\n",
    "from altair.models import build_model\n",
    "import torch\n",
    "\n",
    "# Load run and build model\n",
    "run = alt.load(\"run_abc123\")\n",
    "model = build_model(run.config[\"model\"])\n",
    "\n",
    "# Load weights\n",
    "checkpoint = torch.load(run.best_checkpoint)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Create exporter\n",
    "exporter = ModelExporter(\n",
    "    model=model,\n",
    "    input_shape=(1, 3, 512, 512),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Export to ONNX with custom settings\n",
    "result = exporter.to_onnx(\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "        \"output\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "    },\n",
    "    simplify=True,\n",
    "    validate=True,\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"segmentation\"],\n",
    ")\n",
    "\n",
    "print(f\"Exported to: {result.path}\")\n",
    "print(f\"File size: {result.file_size_mb:.2f} MB\")\n",
    "print(f\"Metadata: {result.metadata}\")\n",
    "\"\"\"\n",
    "print(exporter_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Multiple Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_export_code = \"\"\"\n",
    "from altair.export import ModelExporter\n",
    "\n",
    "exporter = ModelExporter(model, input_shape=(1, 3, 512, 512))\n",
    "\n",
    "# Export to all formats at once\n",
    "results = exporter.export_all(\n",
    "    output_dir=\"exported_models/\",\n",
    "    name=\"segmentation_model\",\n",
    "    formats=[\"onnx\", \"torchscript\"],\n",
    ")\n",
    "\n",
    "for fmt, result in results.items():\n",
    "    print(f\"{fmt}: {result.path} ({result.file_size_mb:.2f} MB)\")\n",
    "\"\"\"\n",
    "print(multi_export_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 Export\n",
    "\n",
    "Export in half precision for faster inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16_code = \"\"\"\n",
    "from altair.export import ModelExporter\n",
    "\n",
    "exporter = ModelExporter(model, input_shape=(1, 3, 512, 512))\n",
    "\n",
    "# Convert to FP16\n",
    "exporter.to_half()\n",
    "\n",
    "# Export\n",
    "result = exporter.to_onnx(\"model_fp16.onnx\")\n",
    "print(f\"FP16 model size: {result.file_size_mb:.2f} MB\")\n",
    "\"\"\"\n",
    "print(fp16_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Exported Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_code = \"\"\"\n",
    "from altair.export import validate_onnx, validate_torchscript\n",
    "\n",
    "# Validate ONNX model\n",
    "results = validate_onnx(\"model.onnx\", input_shape=(1, 3, 512, 512))\n",
    "print(f\"Valid: {results['valid']}\")\n",
    "print(f\"Inference OK: {results['inference_ok']}\")\n",
    "print(f\"Output shape: {results['output_shape']}\")\n",
    "print(f\"File size: {results['file_size_mb']:.2f} MB\")\n",
    "\n",
    "# Validate TorchScript model\n",
    "results = validate_torchscript(\"model.pt\", input_shape=(1, 3, 512, 512))\n",
    "print(f\"Valid: {results['valid']}\")\n",
    "print(f\"Inference OK: {results['inference_ok']}\")\n",
    "\"\"\"\n",
    "print(validate_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Exported Models\n",
    "\n",
    "### ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_inference_code = \"\"\"\n",
    "from altair.export.exporter import ONNXInferenceSession\n",
    "import numpy as np\n",
    "\n",
    "# Create session\n",
    "session = ONNXInferenceSession(\"model.onnx\")\n",
    "\n",
    "# Print model info\n",
    "print(f\"Input shape: {session.input_shape}\")\n",
    "print(f\"Output shape: {session.output_shape}\")\n",
    "\n",
    "# Run inference\n",
    "image = np.random.randn(1, 3, 512, 512).astype(np.float32)\n",
    "output = session(image)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# With actual image\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Preprocess\n",
    "transform = A.Compose([\n",
    "    A.Resize(512, 512),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "image = np.array(Image.open(\"test.png\").convert(\"RGB\"))\n",
    "transformed = transform(image=image)\n",
    "input_tensor = transformed[\"image\"].unsqueeze(0).numpy()\n",
    "\n",
    "# Inference\n",
    "output = session(input_tensor)\n",
    "mask = output.argmax(axis=1).squeeze()  # For multiclass\n",
    "# mask = (output > 0.5).squeeze()  # For binary\n",
    "\"\"\"\n",
    "print(onnx_inference_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_inference_code = \"\"\"\n",
    "from altair.export.exporter import TorchScriptInferenceSession\n",
    "import torch\n",
    "\n",
    "# Create session\n",
    "session = TorchScriptInferenceSession(\"model.pt\", device=\"cuda\")\n",
    "\n",
    "# Run inference\n",
    "image = torch.randn(1, 3, 512, 512)\n",
    "output = session(image)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Move to different device\n",
    "session.to(\"cpu\")\n",
    "\"\"\"\n",
    "print(ts_inference_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command Line Export\n",
    "\n",
    "You can also export using the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_code = \"\"\"\n",
    "# Basic ONNX export\n",
    "altair export --run run_abc123 --output model.onnx\n",
    "\n",
    "# TorchScript export\n",
    "altair export --run run_abc123 --output model.pt --format torchscript\n",
    "\n",
    "# With custom input shape\n",
    "altair export --run run_abc123 --output model.onnx --input-shape 1,3,1024,1024\n",
    "\n",
    "# Fixed input size (no dynamic axes)\n",
    "altair export --run run_abc123 --output model.onnx --no-dynamic\n",
    "\n",
    "# Skip simplification\n",
    "altair export --run run_abc123 --output model.onnx --no-simplify\n",
    "\n",
    "# Different opset version\n",
    "altair export --run run_abc123 --output model.onnx --opset 14\n",
    "\"\"\"\n",
    "print(cli_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Targets\n",
    "\n",
    "### TensorRT (NVIDIA GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorrt_code = \"\"\"\n",
    "# 1. Export to ONNX\n",
    "alt.export(\"run_abc123\", \"model.onnx\", opset_version=17)\n",
    "\n",
    "# 2. Convert to TensorRT engine (using trtexec)\n",
    "# trtexec --onnx=model.onnx --saveEngine=model.trt --fp16\n",
    "\n",
    "# 3. Use TensorRT engine for inference\n",
    "# (See TensorRT documentation)\n",
    "\"\"\"\n",
    "print(tensorrt_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO (Intel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openvino_code = \"\"\"\n",
    "# 1. Export to ONNX\n",
    "alt.export(\"run_abc123\", \"model.onnx\")\n",
    "\n",
    "# 2. Convert with OpenVINO Model Optimizer\n",
    "# mo --input_model model.onnx --output_dir openvino_model\n",
    "\n",
    "# 3. Use OpenVINO runtime for inference\n",
    "# (See OpenVINO documentation)\n",
    "\"\"\"\n",
    "print(openvino_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibTorch C++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libtorch_code = \"\"\"\n",
    "// 1. Export to TorchScript in Python\n",
    "// alt.export(\"run_abc123\", \"model.pt\", format=\"torchscript\")\n",
    "\n",
    "// 2. Load in C++\n",
    "#include <torch/script.h>\n",
    "\n",
    "int main() {\n",
    "    // Load model\n",
    "    torch::jit::script::Module model = torch::jit::load(\"model.pt\");\n",
    "    model.eval();\n",
    "    \n",
    "    // Create input\n",
    "    torch::Tensor input = torch::randn({1, 3, 512, 512});\n",
    "    \n",
    "    // Run inference\n",
    "    torch::Tensor output = model.forward({input}).toTensor();\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "\"\"\"\n",
    "print(libtorch_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Export Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_pipeline = \"\"\"\n",
    "import altair as alt\n",
    "from altair.export import validate_onnx\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "run_id = \"my_run_id\"\n",
    "output_dir = Path(\"deployed_models\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Export to ONNX (dynamic input size)\n",
    "print(\"Exporting to ONNX...\")\n",
    "onnx_path = alt.export(\n",
    "    run_id,\n",
    "    output_dir / \"model.onnx\",\n",
    "    format=\"onnx\",\n",
    "    dynamic_axes=True,\n",
    "    simplify=True,\n",
    "    validate=True,\n",
    ")\n",
    "print(f\"ONNX model: {onnx_path}\")\n",
    "\n",
    "# 2. Export to TorchScript\n",
    "print(\"\\nExporting to TorchScript...\")\n",
    "ts_path = alt.export(\n",
    "    run_id,\n",
    "    output_dir / \"model.pt\",\n",
    "    format=\"torchscript\",\n",
    "    validate=True,\n",
    ")\n",
    "print(f\"TorchScript model: {ts_path}\")\n",
    "\n",
    "# 3. Validate ONNX model\n",
    "print(\"\\nValidating ONNX model...\")\n",
    "results = validate_onnx(onnx_path)\n",
    "print(f\"Valid: {results['valid']}\")\n",
    "print(f\"File size: {results['file_size_mb']:.2f} MB\")\n",
    "\n",
    "# 4. Test inference\n",
    "print(\"\\nTesting inference...\")\n",
    "from altair.export.exporter import ONNXInferenceSession\n",
    "import numpy as np\n",
    "\n",
    "session = ONNXInferenceSession(onnx_path)\n",
    "dummy_input = np.random.randn(1, 3, 512, 512).astype(np.float32)\n",
    "output = session(dummy_input)\n",
    "print(f\"Inference successful! Output shape: {output.shape}\")\n",
    "\n",
    "print(\"\\nExport complete!\")\n",
    "\"\"\"\n",
    "print(complete_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **05_custom_config.ipynb**: Advanced configuration options"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
