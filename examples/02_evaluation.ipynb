{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Evaluation and Analysis\n",
    "\n",
    "This notebook demonstrates how to evaluate a trained model and analyze the results.\n",
    "\n",
    "## What you'll learn:\n",
    "- How to evaluate a model using `alt.evaluate()`\n",
    "- How to interpret evaluation metrics\n",
    "- How to export results to CSV/JSON\n",
    "- How to visualize prediction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Evaluation\n",
    "\n",
    "Evaluate a trained model on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using the run ID (uses validation data from config)\n",
    "# results = alt.evaluate(\"run_abc123\")\n",
    "\n",
    "# Or evaluate on a specific test set\n",
    "# results = alt.evaluate(\"run_abc123\", data=\"path/to/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Metrics\n",
    "\n",
    "The evaluation results contain comprehensive metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example metrics explanation\n",
    "metrics_info = \"\"\"\n",
    "=== Binary Segmentation Metrics ===\n",
    "IoU (Jaccard Index): Intersection / Union of prediction and ground truth\n",
    "    - Range: [0, 1], higher is better\n",
    "    - IoU = TP / (TP + FP + FN)\n",
    "\n",
    "Dice Coefficient: Similar to F1 score, more weight on overlap\n",
    "    - Range: [0, 1], higher is better\n",
    "    - Dice = 2*TP / (2*TP + FP + FN)\n",
    "\n",
    "Precision: How many predicted positives are correct\n",
    "    - Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: How many actual positives are found\n",
    "    - Recall = TP / (TP + FN)\n",
    "\n",
    "=== Multi-class Segmentation Metrics ===\n",
    "mIoU: Mean IoU across all classes\n",
    "mDice: Mean Dice across all classes\n",
    "Pixel Accuracy: Percentage of correctly classified pixels\n",
    "Per-class IoU: IoU for each individual class\n",
    "\n",
    "=== Advanced Metrics (with segmentation-evaluation) ===\n",
    "soft_pq: Soft Panoptic Quality - handles instance segmentation\n",
    "mAP: Mean Average Precision\n",
    "\"\"\"\n",
    "print(metrics_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access metrics from results\n",
    "# (assuming 'results' is available from evaluation)\n",
    "\n",
    "example_code = \"\"\"\n",
    "# Aggregate metrics\n",
    "print(\"=== Aggregate Metrics ===\")\n",
    "for name, value in results.metrics.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# Per-class metrics (for multi-class)\n",
    "print(\"\\n=== Per-class IoU ===\")\n",
    "for cls, iou in results.per_class_metrics.get('IoU', {}).items():\n",
    "    print(f\"Class {cls}: {iou:.4f}\")\n",
    "\n",
    "# Access specific metrics\n",
    "miou = results['mIoU']  # or results.get('mIoU', 0.0)\n",
    "dice = results['mDice']\n",
    "\"\"\"\n",
    "print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Sample Analysis\n",
    "\n",
    "Analyze performance on individual samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample metrics are useful for:\n",
    "# - Finding difficult samples\n",
    "# - Identifying failure cases\n",
    "# - Understanding model behavior\n",
    "\n",
    "example_analysis = \"\"\"\n",
    "# Sort samples by IoU (worst first)\n",
    "sorted_samples = sorted(\n",
    "    results.per_sample_metrics,\n",
    "    key=lambda x: x.get('IoU', x.get('mIoU', 0))\n",
    ")\n",
    "\n",
    "# Print worst 5 samples\n",
    "print(\"=== Worst Performing Samples ===\")\n",
    "for sample in sorted_samples[:5]:\n",
    "    print(f\"Image: {sample.get('image_path', 'N/A')}\")\n",
    "    print(f\"  IoU: {sample.get('IoU', sample.get('mIoU', 0)):.4f}\")\n",
    "    print()\n",
    "\n",
    "# Print best 5 samples\n",
    "print(\"=== Best Performing Samples ===\")\n",
    "for sample in sorted_samples[-5:]:\n",
    "    print(f\"Image: {sample.get('image_path', 'N/A')}\")\n",
    "    print(f\"  IoU: {sample.get('IoU', sample.get('mIoU', 0)):.4f}\")\n",
    "\"\"\"\n",
    "print(example_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "Save results for further analysis or reporting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV (per-sample metrics)\n",
    "# results.to_csv(\"evaluation_results.csv\")\n",
    "\n",
    "# Save to JSON (all metrics)\n",
    "# results.to_json(\"evaluation_results.json\")\n",
    "\n",
    "# Print summary\n",
    "# results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n",
    "\n",
    "Use visualization utilities to inspect predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from altair.utils import (\n",
    "    create_overlay,\n",
    "    create_comparison,\n",
    "    create_error_map,\n",
    "    visualize_prediction,\n",
    "    SampleExporter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overlay visualization\n",
    "example_viz = \"\"\"\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load an image and its prediction\n",
    "image = np.array(Image.open(\"test_image.png\").convert(\"RGB\"))\n",
    "prediction = np.array(Image.open(\"prediction_mask.png\"))\n",
    "ground_truth = np.array(Image.open(\"ground_truth.png\"))\n",
    "\n",
    "# Create overlay (mask on image)\n",
    "overlay = create_overlay(image, prediction, alpha=0.5)\n",
    "Image.fromarray(overlay).save(\"overlay.png\")\n",
    "\n",
    "# Create comparison (image | GT | prediction)\n",
    "comparison = create_comparison(image, ground_truth, prediction)\n",
    "Image.fromarray(comparison).save(\"comparison.png\")\n",
    "\n",
    "# Create error map (green=correct, red=error)\n",
    "error_map = create_error_map(ground_truth, prediction)\n",
    "Image.fromarray(error_map).save(\"error_map.png\")\n",
    "\"\"\"\n",
    "print(example_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full visualization with matplotlib\n",
    "viz_code = \"\"\"\n",
    "# Visualize a single prediction\n",
    "visualize_prediction(\n",
    "    image=image,\n",
    "    ground_truth=ground_truth,\n",
    "    prediction=prediction,\n",
    "    show_error_map=True,\n",
    "    save_path=\"visualization.png\"\n",
    ")\n",
    "\"\"\"\n",
    "print(viz_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Sample Visualizations\n",
    "\n",
    "Export multiple samples with visualizations for review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SampleExporter for batch visualization export\n",
    "export_code = \"\"\"\n",
    "# Create exporter\n",
    "exporter = SampleExporter(\n",
    "    output_dir=\"eval_samples/\",\n",
    "    max_samples=20,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Add samples from evaluation\n",
    "for i, sample in enumerate(results.per_sample_metrics[:20]):\n",
    "    # Load image and masks\n",
    "    image = load_image(sample['image_path'])\n",
    "    prediction = results.predictions[i]  # If stored\n",
    "    ground_truth = load_mask(sample['image_path'])  # Load corresponding GT\n",
    "    \n",
    "    exporter.add_sample(\n",
    "        image=image,\n",
    "        prediction=prediction,\n",
    "        ground_truth=ground_truth,\n",
    "        metrics=sample,\n",
    "        image_path=sample['image_path'],\n",
    "    )\n",
    "\n",
    "# Save summary and grid\n",
    "exporter.save_summary()\n",
    "exporter.save_grid(cols=4)\n",
    "\"\"\"\n",
    "print(export_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Sample Export (All-in-One)\n",
    "\n",
    "You can also export samples directly during evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and export samples in one step\n",
    "# from altair.engine.evaluator import Evaluator\n",
    "# \n",
    "# evaluator = Evaluator(run, checkpoint, store_predictions=True)\n",
    "# results = evaluator.evaluate(\n",
    "#     export_samples=True,\n",
    "#     export_dir=\"eval_samples/\",\n",
    "#     n_export_samples=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Multiple Runs\n",
    "\n",
    "Compare results across different experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_code = \"\"\"\n",
    "# Evaluate multiple runs\n",
    "run_ids = [\"run_abc123\", \"run_def456\", \"run_ghi789\"]\n",
    "all_results = {}\n",
    "\n",
    "for run_id in run_ids:\n",
    "    results = alt.evaluate(run_id, data=\"path/to/test\")\n",
    "    all_results[run_id] = results.metrics\n",
    "\n",
    "# Compare key metrics\n",
    "print(\"=== Comparison ===\")\n",
    "print(f\"{'Run ID':<15} {'mIoU':<10} {'mDice':<10}\")\n",
    "print(\"-\" * 35)\n",
    "for run_id, metrics in all_results.items():\n",
    "    print(f\"{run_id:<15} {metrics['mIoU']:<10.4f} {metrics['mDice']:<10.4f}\")\n",
    "\"\"\"\n",
    "print(compare_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **03_inference.ipynb**: Run predictions on new images\n",
    "- **04_export.ipynb**: Export your model for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
