{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Advanced Configuration\n",
    "\n",
    "This notebook covers advanced configuration options for customizing your experiments.\n",
    "\n",
    "## What you'll learn:\n",
    "- Complete configuration reference\n",
    "- Custom model architectures\n",
    "- Loss function combinations\n",
    "- Advanced augmentation pipelines\n",
    "- Multi-class and regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair.core.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Structure\n",
    "\n",
    "A complete Altair configuration has these sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_config = \"\"\"\n",
    "# ============================================\n",
    "# EXPERIMENT SETTINGS\n",
    "# ============================================\n",
    "experiment:\n",
    "  name: \"my_experiment\"        # Experiment name\n",
    "  project: \"segmentation\"      # Project grouping\n",
    "  seed: 42                     # Random seed for reproducibility\n",
    "  output_dir: \"experiments\"    # Where to save outputs\n",
    "\n",
    "# ============================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================\n",
    "model:\n",
    "  architecture: \"unet\"         # Architecture: unet, unetplusplus\n",
    "  encoder: \"resnet34\"          # Encoder backbone (any timm model)\n",
    "  encoder_weights: \"imagenet\"  # Pretrained weights (or null)\n",
    "  num_classes: 2               # Number of output classes\n",
    "  task: \"binary\"               # Task: binary, multiclass, regression\n",
    "  in_channels: 3               # Input channels (3 for RGB)\n",
    "  decoder_channels: [256, 128, 64, 32, 16]  # Decoder channel sizes\n",
    "  decoder_attention: \"scse\"    # Attention: null, scse\n",
    "\n",
    "# ============================================\n",
    "# DATA CONFIGURATION\n",
    "# ============================================\n",
    "data:\n",
    "  format: \"png\"                # Format: png, coco\n",
    "  train_images: \"data/train/images\"\n",
    "  train_masks: \"data/train/masks\"\n",
    "  val_images: \"data/val/images\"\n",
    "  val_masks: \"data/val/masks\"\n",
    "  batch_size: 8\n",
    "  num_workers: 4\n",
    "  pin_memory: true\n",
    "\n",
    "# ============================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================\n",
    "training:\n",
    "  epochs: 100\n",
    "  lr: 0.001\n",
    "  optimizer: \"adamw\"           # Optimizer: adam, adamw, sgd\n",
    "  weight_decay: 0.01\n",
    "  scheduler: \"cosine\"          # Scheduler: cosine, step, plateau\n",
    "  loss: \"dice\"                 # Loss: bce, ce, dice, focal, combined\n",
    "  gradient_clip: 1.0           # Gradient clipping (null to disable)\n",
    "  amp: true                    # Automatic mixed precision\n",
    "\n",
    "# ============================================\n",
    "# AUGMENTATIONS\n",
    "# ============================================\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: normalize\n",
    "\n",
    "# ============================================\n",
    "# CHECKPOINTING\n",
    "# ============================================\n",
    "checkpointing:\n",
    "  monitor: \"val/Dice\"          # Metric to monitor\n",
    "  mode: \"max\"                  # max or min\n",
    "  save_top_k: 3                # Keep top K checkpoints\n",
    "  save_last: true              # Always save last checkpoint\n",
    "\n",
    "# ============================================\n",
    "# EVALUATION\n",
    "# ============================================\n",
    "evaluation:\n",
    "  thresh: 0.5                  # Threshold for binary prediction\n",
    "  iou_high: 0.5                # Soft PQ upper bound\n",
    "  iou_low: 0.05                # Soft PQ lower bound\n",
    "\n",
    "# ============================================\n",
    "# TRACKING (MLflow)\n",
    "# ============================================\n",
    "tracking:\n",
    "  enabled: true\n",
    "  uri: \"mlruns\"                # MLflow tracking URI\n",
    "\"\"\"\n",
    "print(complete_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "### UNet with Different Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = \"\"\"\n",
    "# ResNet family\n",
    "encoder: \"resnet18\"     # Lightest, fastest\n",
    "encoder: \"resnet34\"     # Good balance\n",
    "encoder: \"resnet50\"     # More capacity\n",
    "encoder: \"resnet101\"    # Even more capacity\n",
    "\n",
    "# EfficientNet family\n",
    "encoder: \"efficientnet_b0\"  # Smallest\n",
    "encoder: \"efficientnet_b3\"  # Medium\n",
    "encoder: \"efficientnet_b5\"  # Large\n",
    "encoder: \"efficientnet_b7\"  # Largest\n",
    "\n",
    "# Other popular encoders\n",
    "encoder: \"mobilenetv3_large_100\"  # Mobile-friendly\n",
    "encoder: \"convnext_tiny\"          # Modern ConvNet\n",
    "encoder: \"swin_tiny_patch4_window7_224\"  # Vision Transformer\n",
    "\n",
    "# Any timm model works!\n",
    "# See: https://huggingface.co/timm\n",
    "\"\"\"\n",
    "print(encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet++ (Dense Skip Connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unetpp_config = \"\"\"\n",
    "model:\n",
    "  architecture: \"unetplusplus\"\n",
    "  encoder: \"resnet34\"\n",
    "  encoder_weights: \"imagenet\"\n",
    "  num_classes: 5\n",
    "  task: \"multiclass\"\n",
    "  \n",
    "  # UNet++ has nested skip connections\n",
    "  # Generally better for small objects\n",
    "\"\"\"\n",
    "print(unetpp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Attention (SCSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_config = \"\"\"\n",
    "model:\n",
    "  architecture: \"unet\"\n",
    "  encoder: \"resnet50\"\n",
    "  encoder_weights: \"imagenet\"\n",
    "  num_classes: 2\n",
    "  task: \"binary\"\n",
    "  \n",
    "  # SCSE: Squeeze-and-Excitation with spatial attention\n",
    "  # Improves feature recalibration\n",
    "  decoder_attention: \"scse\"\n",
    "  \n",
    "  # Or disable attention\n",
    "  # decoder_attention: null\n",
    "\"\"\"\n",
    "print(attention_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_examples = \"\"\"\n",
    "# Binary segmentation\n",
    "training:\n",
    "  loss: \"bce\"          # Binary Cross-Entropy\n",
    "  loss: \"dice\"         # Dice Loss (good for imbalanced)\n",
    "  loss: \"focal\"        # Focal Loss (hard examples)\n",
    "  \n",
    "# Multi-class segmentation\n",
    "training:\n",
    "  loss: \"ce\"           # Cross-Entropy\n",
    "  loss: \"dice\"         # Multi-class Dice\n",
    "  loss: \"focal\"        # Multi-class Focal\n",
    "\n",
    "# Regression\n",
    "training:\n",
    "  loss: \"mse\"          # Mean Squared Error\n",
    "  loss: \"l1\"           # Mean Absolute Error\n",
    "\n",
    "# Combined losses\n",
    "training:\n",
    "  loss: \"combined\"\n",
    "  loss_weights:\n",
    "    bce: 0.5\n",
    "    dice: 0.5\n",
    "    \n",
    "# Or with different weights\n",
    "training:\n",
    "  loss: \"combined\"\n",
    "  loss_weights:\n",
    "    ce: 1.0\n",
    "    dice: 0.5\n",
    "    focal: 0.25\n",
    "\"\"\"\n",
    "print(loss_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_examples = \"\"\"\n",
    "# AdamW (recommended)\n",
    "training:\n",
    "  optimizer: \"adamw\"\n",
    "  lr: 0.001\n",
    "  weight_decay: 0.01\n",
    "\n",
    "# Adam\n",
    "training:\n",
    "  optimizer: \"adam\"\n",
    "  lr: 0.001\n",
    "  weight_decay: 0.0\n",
    "\n",
    "# SGD with momentum\n",
    "training:\n",
    "  optimizer: \"sgd\"\n",
    "  lr: 0.01\n",
    "  momentum: 0.9\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "# Schedulers\n",
    "training:\n",
    "  scheduler: \"cosine\"    # Cosine annealing (recommended)\n",
    "  scheduler: \"step\"      # Step decay\n",
    "  scheduler: \"plateau\"   # Reduce on plateau\n",
    "  scheduler: null        # No scheduler\n",
    "\"\"\"\n",
    "print(optimizer_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_light = \"\"\"\n",
    "# Light augmentation (conservative)\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: normalize\n",
    "\"\"\"\n",
    "print(\"=== Light Augmentation ===\")\n",
    "print(aug_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_medium = \"\"\"\n",
    "# Medium augmentation\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: vertical_flip\n",
    "      p: 0.5\n",
    "    - name: rotate\n",
    "      limit: 30\n",
    "      p: 0.5\n",
    "    - name: brightness_contrast\n",
    "      brightness_limit: 0.2\n",
    "      contrast_limit: 0.2\n",
    "      p: 0.5\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: normalize\n",
    "\"\"\"\n",
    "print(\"=== Medium Augmentation ===\")\n",
    "print(aug_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_heavy = \"\"\"\n",
    "# Heavy augmentation (aggressive)\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: vertical_flip\n",
    "      p: 0.5\n",
    "    - name: rotate\n",
    "      limit: 45\n",
    "      p: 0.7\n",
    "    - name: shift_scale_rotate\n",
    "      shift_limit: 0.1\n",
    "      scale_limit: 0.2\n",
    "      rotate_limit: 0\n",
    "      p: 0.5\n",
    "    - name: brightness_contrast\n",
    "      brightness_limit: 0.3\n",
    "      contrast_limit: 0.3\n",
    "      p: 0.5\n",
    "    - name: hue_saturation\n",
    "      hue_shift_limit: 20\n",
    "      sat_shift_limit: 30\n",
    "      val_shift_limit: 20\n",
    "      p: 0.5\n",
    "    - name: blur\n",
    "      blur_limit: 3\n",
    "      p: 0.3\n",
    "    - name: gaussian_noise\n",
    "      var_limit: [10, 50]\n",
    "      p: 0.3\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: normalize\n",
    "\"\"\"\n",
    "print(\"=== Heavy Augmentation ===\")\n",
    "print(aug_heavy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Types\n",
    "\n",
    "### Binary Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_config = \"\"\"\n",
    "# Binary segmentation (foreground vs background)\n",
    "model:\n",
    "  architecture: \"unet\"\n",
    "  encoder: \"resnet34\"\n",
    "  num_classes: 1        # Single output channel\n",
    "  task: \"binary\"        # Sigmoid activation\n",
    "\n",
    "training:\n",
    "  loss: \"dice\"          # Good for binary\n",
    "\n",
    "evaluation:\n",
    "  thresh: 0.5           # Threshold for prediction\n",
    "\n",
    "checkpointing:\n",
    "  monitor: \"val/Dice\"   # Binary metrics\n",
    "  mode: \"max\"\n",
    "\"\"\"\n",
    "print(binary_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_config = \"\"\"\n",
    "# Multi-class segmentation (multiple classes)\n",
    "model:\n",
    "  architecture: \"unet\"\n",
    "  encoder: \"resnet50\"\n",
    "  num_classes: 10       # Number of classes\n",
    "  task: \"multiclass\"    # Softmax activation\n",
    "\n",
    "training:\n",
    "  loss: \"ce\"            # Cross-entropy\n",
    "  # Or combined\n",
    "  # loss: \"combined\"\n",
    "  # loss_weights:\n",
    "  #   ce: 1.0\n",
    "  #   dice: 0.5\n",
    "\n",
    "checkpointing:\n",
    "  monitor: \"val/mIoU\"   # Multi-class metrics\n",
    "  mode: \"max\"\n",
    "\"\"\"\n",
    "print(multiclass_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_config = \"\"\"\n",
    "# Regression (continuous output)\n",
    "# Example: depth estimation, density maps\n",
    "model:\n",
    "  architecture: \"unet\"\n",
    "  encoder: \"resnet34\"\n",
    "  num_classes: 1        # Single output channel\n",
    "  task: \"regression\"    # No activation (linear output)\n",
    "\n",
    "training:\n",
    "  loss: \"mse\"           # Mean squared error\n",
    "  # Or\n",
    "  # loss: \"l1\"          # Mean absolute error\n",
    "\n",
    "checkpointing:\n",
    "  monitor: \"val/MSE\"    # Regression metrics\n",
    "  mode: \"min\"           # Lower is better\n",
    "\"\"\"\n",
    "print(regression_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats\n",
    "\n",
    "### PNG Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "png_config = \"\"\"\n",
    "# PNG mask format\n",
    "# - Images and masks in separate directories\n",
    "# - Matching filenames (image.png -> image.png)\n",
    "# - Masks are grayscale with class indices as pixel values\n",
    "\n",
    "data:\n",
    "  format: \"png\"\n",
    "  train_images: \"data/train/images\"\n",
    "  train_masks: \"data/train/masks\"\n",
    "  val_images: \"data/val/images\"\n",
    "  val_masks: \"data/val/masks\"\n",
    "\n",
    "# Directory structure:\n",
    "# data/\n",
    "#   train/\n",
    "#     images/\n",
    "#       001.png\n",
    "#       002.png\n",
    "#     masks/\n",
    "#       001.png  (grayscale, values 0-N for N classes)\n",
    "#       002.png\n",
    "#   val/\n",
    "#     ...\n",
    "\"\"\"\n",
    "print(png_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COCO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_config = \"\"\"\n",
    "# COCO format\n",
    "# - Annotations in JSON file\n",
    "# - Supports polygons and RLE masks\n",
    "\n",
    "data:\n",
    "  format: \"coco\"\n",
    "  train_images: \"data/train/images\"\n",
    "  train_annotations: \"data/train/annotations.json\"\n",
    "  val_images: \"data/val/images\"\n",
    "  val_annotations: \"data/val/annotations.json\"\n",
    "\n",
    "# COCO JSON structure:\n",
    "# {\n",
    "#   \"images\": [{\"id\": 1, \"file_name\": \"001.png\", ...}],\n",
    "#   \"annotations\": [{\"id\": 1, \"image_id\": 1, \"category_id\": 1, \"segmentation\": [...]}],\n",
    "#   \"categories\": [{\"id\": 1, \"name\": \"class1\"}, ...]\n",
    "# }\n",
    "\"\"\"\n",
    "print(coco_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Examples\n",
    "\n",
    "### Medical Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_config = \"\"\"\n",
    "experiment:\n",
    "  name: \"tumor_segmentation\"\n",
    "  project: \"medical_imaging\"\n",
    "\n",
    "model:\n",
    "  architecture: \"unet\"\n",
    "  encoder: \"efficientnet_b3\"\n",
    "  encoder_weights: \"imagenet\"\n",
    "  num_classes: 1\n",
    "  task: \"binary\"\n",
    "  decoder_attention: \"scse\"\n",
    "\n",
    "data:\n",
    "  format: \"png\"\n",
    "  train_images: \"data/train/images\"\n",
    "  train_masks: \"data/train/masks\"\n",
    "  val_images: \"data/val/images\"\n",
    "  val_masks: \"data/val/masks\"\n",
    "  batch_size: 16\n",
    "\n",
    "training:\n",
    "  epochs: 200\n",
    "  lr: 0.0001\n",
    "  optimizer: \"adamw\"\n",
    "  scheduler: \"cosine\"\n",
    "  loss: \"combined\"\n",
    "  loss_weights:\n",
    "    dice: 1.0\n",
    "    bce: 0.5\n",
    "\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 512\n",
    "      width: 512\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: vertical_flip\n",
    "      p: 0.5\n",
    "    - name: rotate\n",
    "      limit: 15\n",
    "      p: 0.5\n",
    "    - name: elastic_transform\n",
    "      alpha: 120\n",
    "      sigma: 6\n",
    "      p: 0.3\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 512\n",
    "      width: 512\n",
    "    - name: normalize\n",
    "\"\"\"\n",
    "print(medical_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite Image Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_config = \"\"\"\n",
    "experiment:\n",
    "  name: \"land_cover\"\n",
    "  project: \"remote_sensing\"\n",
    "\n",
    "model:\n",
    "  architecture: \"unetplusplus\"\n",
    "  encoder: \"resnet50\"\n",
    "  encoder_weights: \"imagenet\"\n",
    "  num_classes: 7  # water, forest, urban, etc.\n",
    "  task: \"multiclass\"\n",
    "\n",
    "data:\n",
    "  format: \"png\"\n",
    "  train_images: \"data/train/images\"\n",
    "  train_masks: \"data/train/masks\"\n",
    "  val_images: \"data/val/images\"\n",
    "  val_masks: \"data/val/masks\"\n",
    "  batch_size: 8\n",
    "\n",
    "training:\n",
    "  epochs: 150\n",
    "  lr: 0.001\n",
    "  optimizer: \"adamw\"\n",
    "  scheduler: \"cosine\"\n",
    "  loss: \"combined\"\n",
    "  loss_weights:\n",
    "    ce: 1.0\n",
    "    dice: 0.5\n",
    "\n",
    "augmentations:\n",
    "  train:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: horizontal_flip\n",
    "      p: 0.5\n",
    "    - name: vertical_flip\n",
    "      p: 0.5\n",
    "    - name: rotate\n",
    "      limit: 90\n",
    "      p: 0.5\n",
    "    - name: random_brightness_contrast\n",
    "      p: 0.5\n",
    "    - name: normalize\n",
    "  val:\n",
    "    - name: resize\n",
    "      height: 256\n",
    "      width: 256\n",
    "    - name: normalize\n",
    "\n",
    "checkpointing:\n",
    "  monitor: \"val/mIoU\"\n",
    "  mode: \"max\"\n",
    "\"\"\"\n",
    "print(satellite_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic Configuration\n",
    "\n",
    "You can also build configs programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programmatic_example = \"\"\"\n",
    "from altair.core.config import Config\n",
    "\n",
    "# Build config as dictionary\n",
    "config = {\n",
    "    \"experiment\": {\n",
    "        \"name\": \"test_experiment\",\n",
    "        \"project\": \"testing\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"architecture\": \"unet\",\n",
    "        \"encoder\": \"resnet34\",\n",
    "        \"num_classes\": 2,\n",
    "        \"task\": \"binary\",\n",
    "    },\n",
    "    # ... rest of config\n",
    "}\n",
    "\n",
    "# Validate config\n",
    "validated_config = Config.from_dict(config)\n",
    "\n",
    "# Access config values\n",
    "print(validated_config.model.encoder)\n",
    "print(validated_config.training.lr)\n",
    "\n",
    "# Convert back to dict\n",
    "config_dict = validated_config.to_dict()\n",
    "\n",
    "# Save to YAML\n",
    "validated_config.to_yaml(\"my_config.yaml\")\n",
    "\"\"\"\n",
    "print(programmatic_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key configuration tips:\n",
    "\n",
    "1. **Start simple**: Begin with default settings and iterate\n",
    "2. **Use pretrained encoders**: Almost always improves results\n",
    "3. **Match loss to task**: Dice for imbalanced, CE for balanced\n",
    "4. **Augment appropriately**: Don't over-augment for small datasets\n",
    "5. **Monitor the right metric**: Dice/IoU for segmentation quality\n",
    "6. **Use mixed precision**: Enable `amp: true` for faster training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
